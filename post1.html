<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Blog Post 1</title>
    <link rel="stylesheet" href="styles.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<h1 id="the-bitter-lesson">The Bitter Lesson</h1>
<p>Rich Sutton, one of the pioneers of modern reinforcement learning, detailed an account of the <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">&quot;Bitter Lesson&quot;</a> of machine learning. </p>
<p>In short, he speaks of the power of computation-driven learning. Over the almost 100 years of serious AI research that has happened so much effort has been put into distilling human knowledge down to core concepts or primitives that we can give to machines to make them intelligent. Time and time again, the biggest breakthoughs in ML/AI have not been in perfect representations of knowledge or heuristics about how to learn, but in leveraging computation resources available to allow these software agents to discover the structure in the problems themselves.</p>
<h2 id="learning-without-knowledge">Learning without Knowledge</h2>
<p>The purpose of this session is to show some techiques we have under our belt to tackles problems without incorporating knowledge of the process we want to learn. While using heuristics or priors can great speed up learning on certain problems, we want to show in general we can learn in the most efficient way possible without any of that.</p>
<p>We will structure this post into 3 parts:</p>
<ul>
<li>Stateless, only your actions yield outcomes across independent trails</li>
<li>Stateful, your actions yield outcomes in a context across independent trials</li>
<li>Stateful, your actions yield outcomes in a context over time</li>
</ul>
<p>This allows us to tackle problems with minimum information about the world first, see how we would solve them, and then modify our policies when we have more information to take advantage.</p>
<h2 id="part-1-bandits">Part 1: Bandits</h2>
<p><img src="assets/multi_armed_bandit.png" alt="Bandit"></p>
<p>A bandit problem refers to an environment where only the agents actions will effect the reward they recieve. The example above shows that each action (the arm to pull) has a certain distribution of rewards that can result. The job of the agent is to find out as quickly as possible which action is the best. </p>
<p>What does &quot;best&quot; mean in this context? The metric that is used is called the <strong>Regret</strong>.</p>
<h3 id="regret">Regret</h3>
<p>It is defined as the expected reward an agent lost out on by not choosing the optimal action. To put a bit of terminology to this, we can define the expected reward of an action as $Q(a_i) = E[r | a]$, also called the <strong>Value Function</strong>. If we assume that the bandit arms will give rewards according to a bernoulli random variable $r(a_i) \sim bern(\theta_i)$. Then the regret can be written as:</p>
<p>$$
R(t) = \sum<em>{t = 0}^T (\theta</em>* - Q(a_t))
$$</p>
<p>For example, consider a 2-armed bandit with pull probabilities $(0.3, 0.8)$. The optimal action is to choose the second arm always, $\theta_* = 0.8$. We can compare to action sequences using the regret. Imagine action sequence 1:  $[0, 1, 1, 1, 0, 0]$ and action sequence 2: $[0, 0, 0, 0, 1, 1]$. The regret they accrue can be computed with the above formula:</p>
<p>$$
R_1(t) = (0.8 - 0.3) + (0.8 - 0.8) + (0.8 - 0.8) + (0.8 - 0.8) + (0.8 - 0.3) + (0.8 - 0.3) = 1.5 \
R_2(t) = (0.8 - 0.3) + (0.8 - 0.3) + (0.8 - 0.3) + (0.8 - 0.3) + (0.8 - 0.8) + (0.8 - 0.8) = 2
$$</p>
<p>So the policy that generated the first sequence has the least regret. We seek to find policies with the smallest expected regret.</p>
<pre><code class="lang-python">%load_ext autoreload
%autoreload 2
!pip <span class="hljs-keyword">install</span> -r requirements.txt

<span class="hljs-keyword">import</span> ipywidgets <span class="hljs-keyword">as</span> widgets
<span class="hljs-keyword">from</span> ipywidgets <span class="hljs-keyword">import</span> interact, interactive, <span class="hljs-keyword">fixed</span>, interact_manual
<span class="hljs-keyword">from</span> IPython.display <span class="hljs-keyword">import</span> clear_output, Image


<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> copy <span class="hljs-keyword">import</span> deepcopy
<span class="hljs-keyword">import</span> <span class="hljs-keyword">warnings</span>
warnings.filterwarnings(<span class="hljs-string">"ignore"</span>)

<span class="hljs-keyword">from</span> envs.bandit <span class="hljs-keyword">import</span> Bandit
<span class="hljs-keyword">from</span> envs.context_bandit <span class="hljs-keyword">import</span> ContextualBandit, get_state_probs
<span class="hljs-keyword">from</span> envs.frozen_lake <span class="hljs-keyword">import</span> FrozenLakeEnv

<span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> *
<span class="hljs-keyword">from</span> value_iteration <span class="hljs-keyword">import</span> *
<span class="hljs-keyword">from</span> mcts <span class="hljs-keyword">import</span> *
<span class="hljs-string">'''

```python
"""
Let'</span>s play around <span class="hljs-keyword">with</span> a bandit <span class="hljs-keyword">to</span> see what the problem <span class="hljs-keyword">is</span> like. You can <span class="hljs-keyword">change</span> <span class="hljs-keyword">any</span> <span class="hljs-keyword">of</span> the <span class="hljs-keyword">variables</span> below
<span class="hljs-keyword">to</span> <span class="hljs-keyword">modify</span> how many arms there <span class="hljs-keyword">are</span> <span class="hljs-keyword">or</span> the likelihoods <span class="hljs-keyword">of</span> arms giving reward. Once you run this cell, you will be 
able <span class="hljs-keyword">to</span> interact <span class="hljs-keyword">with</span> <span class="hljs-keyword">some</span> buttons <span class="hljs-keyword">to</span> produce plots <span class="hljs-keyword">on</span> how well you <span class="hljs-keyword">are</span> doing.
<span class="hljs-string">"""
n_arms = 8

# Setup some random arm probabilities
bad_arm_prob = 0.3
good_arm_prob = 0.8
regret = good_arm_prob - bad_arm_prob
probs = np.zeros(n_arms) + bad_arm_prob
probs[np.random.choice(range(n_arms))] = good_arm_prob

# Configure environment
config = {"</span>num_arms<span class="hljs-string">" : n_arms, "</span>probs<span class="hljs-string">": probs}
env = Bandit(config)

# Create fancy UI
buttons = [widgets.Button(description=f'Arm: {i}') for i in range(n_arms)]
cumulative_reward = [0]
def press(button):
    ## Sanitize inputs, clear current output
    clear_output()
    value = int(button.description.split("</span> <span class="hljs-string">")[1])

    ## Step env, keep track of cumulative reward each step
    r = env.step(value)
    cumulative_reward.append(env.cumulative_reward.sum())
    print(f"</span>Reward: {r}, Total steps: {env.visits.sum()}<span class="hljs-string">")

    ## plot
    fig, axs = plt.subplots(1, 3, figsize=(15, 8))
    plot_avg_reward(axs[0], env)
    plot_cumulative_reward(axs[1], cumulative_reward)
    plot_cumulative_regret(axs[2], regret, env)

    plt.legend()
    fig.tight_layout()

    display(buttons)

[buttons[i].on_click(press) for i in range(n_arms)]
buttons = widgets.HBox(buttons)
display(buttons)</span>
</code></pre>
<pre><code><span class="hljs-attribute">Reward</span>: 1, Total steps: 17.0

<span class="http">

<span class="lisp">HBox(<span class="hljs-name">children=</span>(<span class="hljs-name">Button</span>(<span class="hljs-name">description=</span>'Arm: <span class="hljs-number">0</span>', style=ButtonStyle()), Button(<span class="hljs-name">description=</span>'Arm: <span class="hljs-number">1</span>', style=ButtonSty…</span></span>
</code></pre><p><img src="tree_search_files/tree_search_7_2.png" alt="png"></p>
<h2 id="solving-bandit-problems">Solving bandit problems</h2>
<p>As we can see in the regret formula, the most important parameter to estimate is the true average reward an arm will yield.<br>Without any model of how the reward-generating process works, the agent needs to collect information and construct an estimate of the arm averages. </p>
<p><strong>Exploration</strong> and <strong>Exploitation</strong> describes the tradeoff an agent makes while it is trying to make these decisions online. The strategies we will investigate different combinations of how to make decisions under uncertainty:</p>
<ul>
<li>random exploration</li>
<li>minimal exploration, just greedily pull best values we have seen in past</li>
<li>explore with preference towards uncertainty</li>
</ul>
<pre><code class="lang-python"><span class="hljs-string">"""
Pure random exploration will investigate each action with equal likelihood, but never exploits that knowledge to improve its regret.
"""</span>

<span class="hljs-comment"># Run 500 trials of 100 step bandit interactions</span>
n_steps = <span class="hljs-number">100</span>
n_trials = <span class="hljs-number">500</span>
n_arms = <span class="hljs-number">8</span>

<span class="hljs-comment"># Arm probabilities </span>
config = {<span class="hljs-string">"num_arms"</span> : n_arms, <span class="hljs-string">"probs"</span>: (<span class="hljs-number">1</span>/n_arms) * np.arange(n_arms)}
env = Bandit(config)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">random_selection</span><span class="hljs-params">(env)</span>:</span>
    <span class="hljs-keyword">return</span> np.random.randint(env.n_arms)

random = run_simulation(random_selection, env, n_steps=n_steps, n_trials=n_trials)
plot_simulation([random[<span class="hljs-number">1</span>]], random[<span class="hljs-number">0</span>], env)
plt.show()
</code></pre>
<p><img src="tree_search_files/tree_search_9_0.png" alt="png"></p>
<pre><code class="lang-python"><span class="hljs-string">"""
The epsilon greedy action selection algorithm uses the same ideas from random exploration, 
but ensures we exploit that information some fraction of the time. 

Several variations of this algorithm can be constructed, such as using an annealing schedule to drop the
epsilon value over time when we believe our arm average estimates are trustworthy to more greedily apply.
"""</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">e_greedy_selection</span><span class="hljs-params">(env, epsilon)</span>:</span>
    <span class="hljs-keyword">if</span> np.random.rand() &lt; epsilon:
        <span class="hljs-keyword">return</span> np.random.randint(env.n_arms)
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">return</span> np.argmax(env.arm_avgs())
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_egreedy</span><span class="hljs-params">(epsilon)</span>:</span>
    <span class="hljs-string">"Helper method to return a handle for different epsilon values"</span>
    <span class="hljs-keyword">return</span> <span class="hljs-keyword">lambda</span> env: e_greedy_selection(env, epsilon)
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update</span><span class="hljs-params">(epsilon)</span>:</span>
    <span class="hljs-keyword">global</span> egreedy
    selection = create_egreedy(epsilon)
    egreedy = run_simulation(selection, env, n_steps=n_steps, n_trials=n_trials)
    plot_simulation([random[<span class="hljs-number">1</span>], egreedy[<span class="hljs-number">1</span>]], egreedy[<span class="hljs-number">0</span>], env)
    plt.show()
<span class="hljs-comment"># Allows you to interact with different epsilon values and see their effect on solutions relative to the previous random policy</span>
interact_manual(update, epsilon=widgets.FloatSlider(min=<span class="hljs-number">0</span>, max=<span class="hljs-number">1</span>, step=<span class="hljs-number">0.05</span>))
</code></pre>
<pre><code>interactive(<span class="hljs-name">children=</span>(<span class="hljs-name">FloatSlider</span>(<span class="hljs-name">value=0</span>.<span class="hljs-number">0</span>, description='epsilon', max=1.<span class="hljs-number">0</span>, step=0.<span class="hljs-number">05</span>), Button(<span class="hljs-name">description=</span>'R…





&lt;function __main__.update(<span class="hljs-name">epsilon</span>)&gt;
</code></pre><h3 id="ucb-algorithm">UCB Algorithm</h3>
<p>$\epsilon$-Greedy works well for a lot of problems, but requires tuning of the hyperparameter is not really principled in any sort of way. This makes it difficult to say anything about the theoretical guarantees of the algorithm. Using some simple arguments about probability bounds, we can find better tradeoffs of exploitation and exploration.</p>
<p>The idea is to try and estimate how likely a big gap is between our estimated value function and the true one: $Q(a) - \hat{Q}_t(a) \leq \hat{U}_t(a)$. Hoeffding&#39;s inequality can be used if we do not want to assign any prior to the distribution of rewards, works for any bounded distribution, so for a random variable $r$ bounded between $[c_1, c_2]$:</p>
<p>$$P[\frac{1}{n} \sum_{i =0}^{n}(r_i - E[r_i]) \geq \Delta\ ] \leq e^{- \frac{2n\Delta^2}{(c_2 - c_1)^2}}$$</p>
<p>This looks eerily similar to our estimates of our value function, $Q(a) = \frac{1}{n} \sum_{i=0}^{n} r_i$:</p>
<p>$$P[Q_t - \hat{Q_t} \geq \Delta ] \leq e^{-\frac{2n(a)\Delta^2}{(c_2 - c_1)^2}}$$</p>
<h2 id="ucb-1-heuristic">UCB 1 Heuristic</h2>
<p>We see that then our upper bound on the true reward can be given by $\Delta = \hat{U_t}(a)$. And if we make the simplifying assumption that the reward is bounded to be on the interval $(0, 1)$. We can then for any value of the probability of deviation, $p$, we can solve for what we expect the upper bound on that deviation to be:</p>
<p>$$
p \approx e^{-2n(a)U_t(a)^2} \
U_t(a) = \sqrt{\frac{- \log p}{2 n (a)}}
$$</p>
<p>Most of the time in practice, we use a heuristic to drop this $p$ over time, typically $p=t^{-4}$. This yields the most commonly used version of UCB, UCB-1:</p>
<p>$$
U_t(a) = \sqrt{\frac{2 \log t}{n(a)}} \
a_t = argmax_a [Q(a) + U_t(a)]
$$</p>
<pre><code class="lang-python"><span class="hljs-string">"""
UCB-1 action selection gives us a heuristic that works relatively well as long as the arm probabilities are not too close.
"""</span>

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ucb_selection</span><span class="hljs-params">(env)</span>:</span>
    ucb = np.sqrt((<span class="hljs-number">2</span> * np.log(env.visits.sum())) / env.visits)
    mod_avgs = env.arm_avgs() + ucb
    <span class="hljs-keyword">return</span> np.argmax(mod_avgs)

ucb = run_simulation(ucb_selection, env, n_steps=n_steps, n_trials=n_trials)
plot_simulation([random[<span class="hljs-number">1</span>], egreedy[<span class="hljs-number">1</span>], ucb[<span class="hljs-number">1</span>]], ucb[<span class="hljs-number">0</span>], env)
plt.show()
</code></pre>
<p><img src="tree_search_files/tree_search_15_0.png" alt="png"></p>
<h2 id="part-2-contextual-bandits">Part 2: Contextual Bandits</h2>
<p>The next class of environments we will look at are referred to as &quot;Contexual&quot; bandit problems. This refers to an observation we get to perform before choosing our action. The observation allows us to have &quot;context&quot; of the situation to use in choosing a useful action.</p>
<p><img src="assets/3_contextual_bandits.max-1500x1500.png" alt=""></p>
<p>Examples of these types of problems are very common in recommender systems. We can think of a user having a context (or in a profile) that we can review before making a decision as to what ad we should serve or movie we can recommend.</p>
<pre><code class="lang-python"><span class="hljs-string">"""
Let's play around with a bandit to see what the problem is like. You can change any of the variables below
to modify how many arms there are or the likelihoods of arms giving reward. Once you run this cell, you will be 
able to interact with some buttons to produce plots on how well you are doing.
"""</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">draw_state</span><span class="hljs-params">(state)</span>:</span>
        <span class="hljs-comment"># Use a colored box to show context</span>
    <span class="hljs-keyword">if</span> (state == <span class="hljs-number">0</span>):
        <span class="hljs-keyword">return</span> widgets.HTML(value=<span class="hljs-string">"&lt;div style='background-color: green ; padding: 10px; border: 1px solid green;'&gt;"</span>)
    <span class="hljs-keyword">else</span>: 
        <span class="hljs-keyword">return</span> widgets.HTML(value=<span class="hljs-string">"&lt;div style='background-color: red ; padding: 10px; border: 1px solid red;'&gt;"</span>)

n_arms = <span class="hljs-number">8</span>

n_arms = <span class="hljs-number">2</span>
n_states = <span class="hljs-number">2</span>

<span class="hljs-comment">## Red: arm 1, Green: arm 0</span>
state_probs = np.array([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.8</span>], [<span class="hljs-number">0.8</span>, <span class="hljs-number">0.1</span>]])
<span class="hljs-comment"># state_probs = get_state_probs(n_states, n_arms)</span>
config = {
    <span class="hljs-string">"num_arms"</span> : n_arms,
    <span class="hljs-string">"num_states"</span> : n_states,
    <span class="hljs-string">"state_probs"</span>: state_probs
}
env = ContextualBandit(config)
state = env.reset()
print(f<span class="hljs-string">"State: {state}"</span>)
cumulative_reward = [<span class="hljs-number">0</span>]

<span class="hljs-comment"># Create fancy UI</span>
buttons = [widgets.Button(description=f<span class="hljs-string">'Arm: {i}'</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(n_arms)]
box = draw_state(state)
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">press</span><span class="hljs-params">(button)</span>:</span>
    <span class="hljs-keyword">global</span> state, cumulative_reward

    <span class="hljs-comment">## Sanitize inputs, clear current output</span>
    clear_output()
    value = int(button.description.split(<span class="hljs-string">" "</span>)[<span class="hljs-number">1</span>])

    <span class="hljs-comment">## Step env, keep track of cumulative reward each step</span>
    state, reward = env.step(value)
    cumulative_reward.append(env.cumulative_reward.sum())
    print(f<span class="hljs-string">"New State: {state}, Reward: {reward}, Total steps: {env.visits.sum()}"</span>)


    fig, axs = plt.subplots(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, figsize=(<span class="hljs-number">15</span>, <span class="hljs-number">8</span>))


    plot_avg_reward_contextual(axs[<span class="hljs-number">0</span>], env)
    plot_cumulative_reward(axs[<span class="hljs-number">1</span>], cumulative_reward)
    plot_cumulative_regret(axs[<span class="hljs-number">2</span>], <span class="hljs-number">0.7</span>, env)

    box = draw_state(state)
    fig.tight_layout()
    display(buttons)
    display(box)

[buttons[i].on_click(press) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(n_arms)]
buttons = widgets.HBox(buttons)
display(buttons)
display(box)
</code></pre>
<pre><code>New State: <span class="hljs-number">0</span>, Reward: <span class="hljs-number">1</span>, Total steps: <span class="hljs-number">13.0</span>



<span class="hljs-function"><span class="hljs-title">HBox</span><span class="hljs-params">(children=(Button(description=<span class="hljs-string">'Arm: 0'</span>, style=ButtonStyle()</span></span>), Button(description=<span class="hljs-string">'Arm: 1'</span>, style=ButtonSty…



<span class="hljs-function"><span class="hljs-title">HTML</span><span class="hljs-params">(value=<span class="hljs-string">"&lt;div style='background-color: green ; padding: 10px; border: 1px solid green;'&gt;"</span>)</span></span>
</code></pre><p><img src="tree_search_files/tree_search_17_3.png" alt="png"></p>
<pre><code class="lang-python"><span class="hljs-string">"""
Similar as before, using no exploitation gives good coverage across the state/action space.

Note: the heat map on the left is normalized by row to show what the pulls were across all states.
"""</span>
n_steps = <span class="hljs-number">100</span>
n_trials = <span class="hljs-number">500</span>
n_arms = <span class="hljs-number">5</span>
n_states = <span class="hljs-number">10</span>

n_steps *= n_states <span class="hljs-comment"># more states dilutes each interaction</span>
state_probs = get_state_probs(n_states, n_arms)
config = {
    <span class="hljs-string">"num_arms"</span> : n_arms,
    <span class="hljs-string">"num_states"</span> : n_states,
    <span class="hljs-string">"state_probs"</span>: state_probs
}
env = ContextualBandit(config)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">random_selection</span><span class="hljs-params">(env)</span>:</span>
    <span class="hljs-keyword">return</span> np.random.randint(env.n_arms)

random = run_simulation(random_selection, env, n_steps=n_steps, n_trials=n_trials)
plot_contextual_simulation([random[<span class="hljs-number">1</span>]], random[<span class="hljs-number">0</span>], env)
plt.show()
</code></pre>
<p><img src="tree_search_files/tree_search_18_0.png" alt="png"></p>
<pre><code class="lang-python"><span class="hljs-string">"""
Similar to epsilon greedy as before, we keep track of the average rewards for each state for all the arms. 
We can then exploit this knowledge every so often dependent on the value of epsilon.
"""</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">e_greedy_selection</span><span class="hljs-params">(env, epsilon)</span>:</span>
    <span class="hljs-keyword">if</span> np.random.rand() &lt; epsilon:
        <span class="hljs-keyword">return</span> np.random.randint(env.n_arms)
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">return</span> np.argmax(env.arm_avgs(env.state))

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_egreedy</span><span class="hljs-params">(epsilon)</span>:</span>
    <span class="hljs-keyword">return</span> <span class="hljs-keyword">lambda</span> env: e_greedy_selection(env, epsilon)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update</span><span class="hljs-params">(epsilon)</span>:</span>
    <span class="hljs-keyword">global</span> egreedy
    selection = create_egreedy(epsilon)
    egreedy = run_simulation(selection, env, n_steps=n_steps, n_trials=n_trials)
    plot_contextual_simulation([random[<span class="hljs-number">1</span>], egreedy[<span class="hljs-number">1</span>]], egreedy[<span class="hljs-number">0</span>], env)
    plt.show()

interact_manual(update, epsilon=widgets.FloatSlider(min=<span class="hljs-number">0</span>, max=<span class="hljs-number">1</span>, step=<span class="hljs-number">0.05</span>))
</code></pre>
<pre><code>interactive(<span class="hljs-name">children=</span>(<span class="hljs-name">FloatSlider</span>(<span class="hljs-name">value=0</span>.<span class="hljs-number">0</span>, description='epsilon', max=1.<span class="hljs-number">0</span>, step=0.<span class="hljs-number">05</span>), Button(<span class="hljs-name">description=</span>'R…





&lt;function __main__.update(<span class="hljs-name">epsilon</span>)&gt;
</code></pre><pre><code class="lang-python"><span class="hljs-string">"""
As with the other solutions we can simply construct similar estimates as the standard bandit problem, but now stateful.
"""</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ucb_selection</span><span class="hljs-params">(env)</span>:</span>
    ucb = np.sqrt((<span class="hljs-number">2</span> * np.log(env.visits[env.state, :].sum())) / env.visits[env.state, :])
    mod_avgs = env.arm_avgs(env.state) + ucb
    <span class="hljs-keyword">return</span> np.argmax(mod_avgs)

ucb = run_simulation(ucb_selection, env, n_steps=n_steps, n_trials=n_trials)
plot_contextual_simulation([random[<span class="hljs-number">1</span>], egreedy[<span class="hljs-number">1</span>], ucb[<span class="hljs-number">1</span>]], ucb[<span class="hljs-number">0</span>], env)
plt.show()
</code></pre>
<p><img src="tree_search_files/tree_search_20_0.png" alt="png"></p>
<h2 id="part-3-markov-decision-processes-mdps-">Part 3: Markov Decision Processes (MDPs)</h2>
<p>We have been building up the problem description in a more and more complex manner over the course of this session.</p>
<p>Our agents have only seen a state, took an action, and then recieved a reward for how they acted. There was no notion of time in any of these algorithms and so the agents actions had no effect on the future. Real decision making processes have some degree of effect on the payoff you get. We can model these systems with MDPs.</p>
<p>MDPs are a general framework for dealing with decision making over some sort of horizon (possibly even infinite).</p>
<p>For these next set of experiments, its helps intuition to think about making decisions in a spatial manner. </p>
<pre><code class="lang-python">env = FrozenLakeEnv(<span class="hljs-number">8</span>)
<span class="hljs-keyword">state</span> = env.reset()
env.render()
</code></pre>
<p><img src="tree_search_files/tree_search_23_0.png" alt="png"></p>
<h2 id="value-functions">Value Functions</h2>
<p>Just as with the bandit problems, a value function helped us decide which actions were high quality and those that were not. The only difference between value functions in MDPs and those in Bandits is that added consideration of time. To formalize a bit: </p>
<p>$$
Q<em>t(s, a) = E [ \sum</em>{k = 0}^{\infty} R_{t + k + 1} | S_t = s, A_t = a]<br>$$</p>
<p>This says the value of a state and an action is what expect the rest of the rewards to be for the rest of time. This is a powerful idea, because it allows us to turn each step of decision making back into a bandit problem. </p>
<p>Let&#39;s take a look at a value function for the frozen lake environment.</p>
<pre><code class="lang-python">"""
The value function can be computed using dynamic programming. If we <span class="hljs-keyword">start</span> <span class="hljs-keyword">from</span> the goal state, we can <span class="hljs-keyword">iterate</span> backwards <span class="hljs-keyword">until</span> we reach the goal.
<span class="hljs-keyword">In</span> <span class="hljs-keyword">order</span> <span class="hljs-keyword">to</span> encourage an <span class="hljs-keyword">agent</span> <span class="hljs-keyword">to</span> <span class="hljs-keyword">get</span> <span class="hljs-keyword">to</span> the goal <span class="hljs-keyword">as</span> soon <span class="hljs-keyword">as</span> possible, we discount the reward a state gets the longer it takes <span class="hljs-keyword">to</span> <span class="hljs-keyword">get</span> there.
<span class="hljs-string">"""

value_fcn = value_iteration(env, gamma=0.9)
env.render()
plot_value_function(value_fcn, env)</span>
</code></pre>
<p><img src="tree_search_files/tree_search_25_0.png" alt="png"></p>
<p><img src="tree_search_files/tree_search_25_1.png" alt="png"></p>
<h1 id="monte-carlo-tree-search">Monte Carlo Tree Search</h1>
<p>We have discussed MCTS on this team quite a bit, but how does it fit into all of this?
<img src="assets/EieiQ.png" alt=""></p>
<p>The algorithm essentially builds up this value function using a combination of tree search + random play (monte carlo simulation)</p>
<pre><code class="lang-python"><span class="hljs-meta"># mcts = MCTS(env)</span>
<span class="hljs-meta"># env.get_action(mcts.next_move(state))</span>

<span class="hljs-meta"># print("Value computed during tree search: ")</span>
<span class="hljs-meta"># plot_value_function(mcts.value_fcn(), env)</span>

print(<span class="hljs-string">"Visitations for each state during tree search: "</span>)
plot_value_function(mcts.visitation_fcn(), env)
</code></pre>
<pre><code>Visitations <span class="hljs-keyword">for</span> each <span class="hljs-keyword">state</span> during tree search: 
</code></pre><p><img src="tree_search_files/tree_search_27_1.png" alt="png"></p>
<pre><code class="lang-python"><span class="hljs-comment"># state = env.reset()</span>
done = False

<span class="hljs-comment"># env = FrozenLakeEnv(10)</span>
<span class="hljs-keyword">state</span> = env.reset()
env.render()
steps = <span class="hljs-number">0</span>
while not done:
    mcts = MCTS(env)
    action = mcts.next_move(<span class="hljs-keyword">state</span>)
    <span class="hljs-keyword">state</span>, reward, done, _ = env.step(action)
    steps += <span class="hljs-number">1</span>

    env.render(save=True, ind=steps)
    <span class="hljs-comment"># plot_value_function(mcts.value_fcn(), env)</span>

create_gif(<span class="hljs-string">"assets/frozen_lake/step_"</span>, steps)

print(f<span class="hljs-string">"Success! {steps} moves"</span>)

clear_output(wait=True)
plt.clf()
Image(url='assets/solution.gif')
</code></pre>
<p><img src="assets/solution.gif"/></p>
<pre><code>&lt;Figure size <span class="hljs-number">432</span>x288 with <span class="hljs-number">0</span> Axes&gt;
</code></pre><pre><code class="lang-python">
</code></pre>

</body>
</html>

