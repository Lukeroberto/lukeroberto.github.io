<DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Bandit Problems</title>
        <link rel="stylesheet" href="../style.css" />
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
    <div class="site-content">
        <header>
            <h1>Bandit Problems</h1>
            <nav>
                                <a href="../index.html">Back to Home</a>             </nav>
        </header>
        <main>
            <h1 id="the-bitter-lesson">The Bitter Lesson</h1>
            <p>Rich Sutton, one of the pioneers of modern reinforcement learning, detailed an account of the <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">“Bitter Lesson”</a> of machine learning.</p>
            <p>In short, he speaks of the power of computation-driven learning. Over the almost 100 years of serious AI research that has happened so much effort has been put into distilling human knowledge down to core concepts or primitives that we can give to machines to make them intelligent. Time and time again, the biggest breakthoughs in ML/AI have not been in perfect representations of knowledge or heuristics about how to learn, but in leveraging computation resources available to allow these software agents to discover the structure in the problems themselves.</p>
            <h2 id="learning-without-knowledge">Learning without Knowledge</h2>
            <p>The purpose of this session is to show some techiques we have under our belt to tackles problems without incorporating knowledge of the process we want to learn. While using heuristics or priors can great speed up learning on certain problems, we want to show in general we can learn in the most efficient way possible without any of that.</p>
            <p>We will structure this post into 3 parts:</p>
            <ul>
            <li>Stateless, only your actions yield outcomes across independent trails</li>
            <li>Stateful, your actions yield outcomes in a context across independent trials</li>
            <li>Stateful, your actions yield outcomes in a context over time</li>
            </ul>
            <p>This allows us to tackle problems with minimum information about the world first, see how we would solve them, and then modify our policies when we have more information to take advantage.</p>
            <h2 id="part-1-bandits">Part 1: Bandits</h2>
            <figure>
            <img src="assets/multi_armed_bandit.png" alt="" /><figcaption>Bandit</figcaption>
            </figure>
            <p>A bandit problem refers to an environment where only the agents actions will effect the reward they recieve. The example above shows that each action (the arm to pull) has a certain distribution of rewards that can result. The job of the agent is to find out as quickly as possible which action is the best.</p>
            <p>What does “best” mean in this context? The metric that is used is called the <strong>Regret</strong>.</p>
            <h3 id="regret">Regret</h3>
            <p>It is defined as the expected reward an agent lost out on by not choosing the optimal action. To put a bit of terminology to this, we can define the expected reward of an action as <span class="math inline">\(Q(a_i) = E[r | a]\)</span>, also called the <strong>Value Function</strong>. If we assume that the bandit arms will give rewards according to a bernoulli random variable <span class="math inline">\(r(a_i) \sim bern(\theta_i)\)</span>. Then the regret can be written as:</p>
            <p><span class="math display">\[
            R(t) = \sum_{t = 0}^T (\theta_* - Q(a_t))
            \]</span></p>
            <p>For example, consider a 2-armed bandit with pull probabilities <span class="math inline">\((0.3, 0.8)\)</span>. The optimal action is to choose the second arm always, <span class="math inline">\(\theta_* = 0.8\)</span>. We can compare to action sequences using the regret. Imagine action sequence 1: <span class="math inline">\([0, 1, 1, 1, 0, 0]\)</span> and action sequence 2: <span class="math inline">\([0, 0, 0, 0, 1, 1]\)</span>. The regret they accrue can be computed with the above formula:</p>
            <p><span class="math display">\[
            R_1(t) = (0.8 - 0.3) + (0.8 - 0.8) + (0.8 - 0.8) + (0.8 - 0.8) + (0.8 - 0.3) + (0.8 - 0.3) = 1.5 \\
            R_2(t) = (0.8 - 0.3) + (0.8 - 0.3) + (0.8 - 0.3) + (0.8 - 0.3) + (0.8 - 0.8) + (0.8 - 0.8) = 2
            \]</span></p>
            <p>So the policy that generated the first sequence has the least regret. We seek to find policies with the smallest expected regret.</p>
            <div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="co">&quot;&quot;&quot;</span></span>
            <span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a><span class="co">Let&#39;s play around with a bandit to see what the problem is like. You can change any of the variables below</span></span>
            <span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a><span class="co">to modify how many arms there are or the likelihoods of arms giving reward. Once you run this cell, you will be </span></span>
            <span id="cb1-4"><a href="#cb1-4" aria-hidden="true"></a><span class="co">able to interact with some buttons to produce plots on how well you are doing.</span></span>
            <span id="cb1-5"><a href="#cb1-5" aria-hidden="true"></a><span class="co">&quot;&quot;&quot;</span></span>
            <span id="cb1-6"><a href="#cb1-6" aria-hidden="true"></a>n_arms <span class="op">=</span> <span class="dv">8</span></span>
            <span id="cb1-7"><a href="#cb1-7" aria-hidden="true"></a></span>
            <span id="cb1-8"><a href="#cb1-8" aria-hidden="true"></a><span class="co"># Setup some random arm probabilities</span></span>
            <span id="cb1-9"><a href="#cb1-9" aria-hidden="true"></a>bad_arm_prob <span class="op">=</span> <span class="fl">0.3</span></span>
            <span id="cb1-10"><a href="#cb1-10" aria-hidden="true"></a>good_arm_prob <span class="op">=</span> <span class="fl">0.8</span></span>
            <span id="cb1-11"><a href="#cb1-11" aria-hidden="true"></a>regret <span class="op">=</span> good_arm_prob <span class="op">-</span> bad_arm_prob</span>
            <span id="cb1-12"><a href="#cb1-12" aria-hidden="true"></a>probs <span class="op">=</span> np.zeros(n_arms) <span class="op">+</span> bad_arm_prob</span>
            <span id="cb1-13"><a href="#cb1-13" aria-hidden="true"></a>probs[np.random.choice(<span class="bu">range</span>(n_arms))] <span class="op">=</span> good_arm_prob</span>
            <span id="cb1-14"><a href="#cb1-14" aria-hidden="true"></a></span>
            <span id="cb1-15"><a href="#cb1-15" aria-hidden="true"></a><span class="co"># Configure environment</span></span>
            <span id="cb1-16"><a href="#cb1-16" aria-hidden="true"></a>config <span class="op">=</span> {<span class="st">&quot;num_arms&quot;</span> : n_arms, <span class="st">&quot;probs&quot;</span>: probs}</span>
            <span id="cb1-17"><a href="#cb1-17" aria-hidden="true"></a>env <span class="op">=</span> Bandit(config)</span>
            <span id="cb1-18"><a href="#cb1-18" aria-hidden="true"></a></span>
            <span id="cb1-19"><a href="#cb1-19" aria-hidden="true"></a><span class="co"># Create fancy UI</span></span>
            <span id="cb1-20"><a href="#cb1-20" aria-hidden="true"></a>buttons <span class="op">=</span> [widgets.Button(description<span class="op">=</span><span class="ss">f&#39;Arm: </span><span class="sc">{i}</span><span class="ss">&#39;</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_arms)]</span>
            <span id="cb1-21"><a href="#cb1-21" aria-hidden="true"></a>cumulative_reward <span class="op">=</span> [<span class="dv">0</span>]</span>
            <span id="cb1-22"><a href="#cb1-22" aria-hidden="true"></a><span class="kw">def</span> press(button):</span>
            <span id="cb1-23"><a href="#cb1-23" aria-hidden="true"></a>    <span class="co">## Sanitize inputs, clear current output</span></span>
            <span id="cb1-24"><a href="#cb1-24" aria-hidden="true"></a>    clear_output()</span>
            <span id="cb1-25"><a href="#cb1-25" aria-hidden="true"></a>    value <span class="op">=</span> <span class="bu">int</span>(button.description.split(<span class="st">&quot; &quot;</span>)[<span class="dv">1</span>])</span>
            <span id="cb1-26"><a href="#cb1-26" aria-hidden="true"></a>    </span>
            <span id="cb1-27"><a href="#cb1-27" aria-hidden="true"></a>    <span class="co">## Step env, keep track of cumulative reward each step</span></span>
            <span id="cb1-28"><a href="#cb1-28" aria-hidden="true"></a>    r <span class="op">=</span> env.step(value)</span>
            <span id="cb1-29"><a href="#cb1-29" aria-hidden="true"></a>    cumulative_reward.append(env.cumulative_reward.<span class="bu">sum</span>())</span>
            <span id="cb1-30"><a href="#cb1-30" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Reward: </span><span class="sc">{r}</span><span class="ss">, Total steps: </span><span class="sc">{</span>env<span class="sc">.</span>visits<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">}</span><span class="ss">&quot;</span>)</span>
            <span id="cb1-31"><a href="#cb1-31" aria-hidden="true"></a>    </span>
            <span id="cb1-32"><a href="#cb1-32" aria-hidden="true"></a>    <span class="co">## plot</span></span>
            <span id="cb1-33"><a href="#cb1-33" aria-hidden="true"></a>    fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">8</span>))</span>
            <span id="cb1-34"><a href="#cb1-34" aria-hidden="true"></a>    plot_avg_reward(axs[<span class="dv">0</span>], env)</span>
            <span id="cb1-35"><a href="#cb1-35" aria-hidden="true"></a>    plot_cumulative_reward(axs[<span class="dv">1</span>], cumulative_reward)</span>
            <span id="cb1-36"><a href="#cb1-36" aria-hidden="true"></a>    plot_cumulative_regret(axs[<span class="dv">2</span>], regret, env)</span>
            <span id="cb1-37"><a href="#cb1-37" aria-hidden="true"></a></span>
            <span id="cb1-38"><a href="#cb1-38" aria-hidden="true"></a>    plt.legend()</span>
            <span id="cb1-39"><a href="#cb1-39" aria-hidden="true"></a>    fig.tight_layout()</span>
            <span id="cb1-40"><a href="#cb1-40" aria-hidden="true"></a>    </span>
            <span id="cb1-41"><a href="#cb1-41" aria-hidden="true"></a>    display(buttons)</span>
            <span id="cb1-42"><a href="#cb1-42" aria-hidden="true"></a></span>
            <span id="cb1-43"><a href="#cb1-43" aria-hidden="true"></a>[buttons[i].on_click(press) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_arms)]</span>
            <span id="cb1-44"><a href="#cb1-44" aria-hidden="true"></a>buttons <span class="op">=</span> widgets.HBox(buttons)</span>
            <span id="cb1-45"><a href="#cb1-45" aria-hidden="true"></a>display(buttons)</span></code></pre></div>
            <pre><code>Reward: 1, Total steps: 17.0



            HBox(children=(Button(description=&#39;Arm: 0&#39;, style=ButtonStyle()), Button(description=&#39;Arm: 1&#39;, style=ButtonSty…</code></pre>
            <figure>
            <img src="tree_search_files/tree_search_7_2.png" alt="" /><figcaption>png</figcaption>
            </figure>
            <h2 id="solving-bandit-problems">Solving bandit problems</h2>
            <p>As we can see in the regret formula, the most important parameter to estimate is the true average reward an arm will yield.<br />
            Without any model of how the reward-generating process works, the agent needs to collect information and construct an estimate of the arm averages.</p>
            <p><strong>Exploration</strong> and <strong>Exploitation</strong> describes the tradeoff an agent makes while it is trying to make these decisions online. The strategies we will investigate different combinations of how to make decisions under uncertainty: - random exploration - minimal exploration, just greedily pull best values we have seen in past - explore with preference towards uncertainty</p>
            <div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a><span class="co">&quot;&quot;&quot;</span></span>
            <span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a><span class="co">Pure random exploration will investigate each action with equal likelihood, but never exploits that knowledge to improve its regret.</span></span>
            <span id="cb3-3"><a href="#cb3-3" aria-hidden="true"></a><span class="co">&quot;&quot;&quot;</span></span>
            <span id="cb3-4"><a href="#cb3-4" aria-hidden="true"></a></span>
            <span id="cb3-5"><a href="#cb3-5" aria-hidden="true"></a><span class="co"># Run 500 trials of 100 step bandit interactions</span></span>
            <span id="cb3-6"><a href="#cb3-6" aria-hidden="true"></a>n_steps <span class="op">=</span> <span class="dv">100</span></span>
            <span id="cb3-7"><a href="#cb3-7" aria-hidden="true"></a>n_trials <span class="op">=</span> <span class="dv">500</span></span>
            <span id="cb3-8"><a href="#cb3-8" aria-hidden="true"></a>n_arms <span class="op">=</span> <span class="dv">8</span></span>
            <span id="cb3-9"><a href="#cb3-9" aria-hidden="true"></a></span>
            <span id="cb3-10"><a href="#cb3-10" aria-hidden="true"></a><span class="co"># Arm probabilities </span></span>
            <span id="cb3-11"><a href="#cb3-11" aria-hidden="true"></a>config <span class="op">=</span> {<span class="st">&quot;num_arms&quot;</span> : n_arms, <span class="st">&quot;probs&quot;</span>: (<span class="dv">1</span><span class="op">/</span>n_arms) <span class="op">*</span> np.arange(n_arms)}</span>
            <span id="cb3-12"><a href="#cb3-12" aria-hidden="true"></a>env <span class="op">=</span> Bandit(config)</span>
            <span id="cb3-13"><a href="#cb3-13" aria-hidden="true"></a></span>
            <span id="cb3-14"><a href="#cb3-14" aria-hidden="true"></a><span class="kw">def</span> random_selection(env):</span>
            <span id="cb3-15"><a href="#cb3-15" aria-hidden="true"></a>    <span class="cf">return</span> np.random.randint(env.n_arms)</span>
            <span id="cb3-16"><a href="#cb3-16" aria-hidden="true"></a></span>
            <span id="cb3-17"><a href="#cb3-17" aria-hidden="true"></a>random <span class="op">=</span> run_simulation(random_selection, env, n_steps<span class="op">=</span>n_steps, n_trials<span class="op">=</span>n_trials)</span>
            <span id="cb3-18"><a href="#cb3-18" aria-hidden="true"></a>plot_simulation([random[<span class="dv">1</span>]], random[<span class="dv">0</span>], env)</span>
            <span id="cb3-19"><a href="#cb3-19" aria-hidden="true"></a>plt.show()</span></code></pre></div>
            <figure>
            <img src="tree_search_files/tree_search_9_0.png" alt="" /><figcaption>png</figcaption>
            </figure>
            <div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true"></a><span class="co">&quot;&quot;&quot;</span></span>
            <span id="cb4-2"><a href="#cb4-2" aria-hidden="true"></a><span class="co">The epsilon greedy action selection algorithm uses the same ideas from random exploration, </span></span>
            <span id="cb4-3"><a href="#cb4-3" aria-hidden="true"></a><span class="co">but ensures we exploit that information some fraction of the time. </span></span>
            <span id="cb4-4"><a href="#cb4-4" aria-hidden="true"></a></span>
            <span id="cb4-5"><a href="#cb4-5" aria-hidden="true"></a><span class="co">Several variations of this algorithm can be constructed, such as using an annealing schedule to drop the</span></span>
            <span id="cb4-6"><a href="#cb4-6" aria-hidden="true"></a><span class="co">epsilon value over time when we believe our arm average estimates are trustworthy to more greedily apply.</span></span>
            <span id="cb4-7"><a href="#cb4-7" aria-hidden="true"></a><span class="co">&quot;&quot;&quot;</span></span>
            <span id="cb4-8"><a href="#cb4-8" aria-hidden="true"></a><span class="kw">def</span> e_greedy_selection(env, epsilon):</span>
            <span id="cb4-9"><a href="#cb4-9" aria-hidden="true"></a>    <span class="cf">if</span> np.random.rand() <span class="op">&lt;</span> epsilon:</span>
            <span id="cb4-10"><a href="#cb4-10" aria-hidden="true"></a>        <span class="cf">return</span> np.random.randint(env.n_arms)</span>
            <span id="cb4-11"><a href="#cb4-11" aria-hidden="true"></a>    <span class="cf">else</span>:</span>
            <span id="cb4-12"><a href="#cb4-12" aria-hidden="true"></a>        <span class="cf">return</span> np.argmax(env.arm_avgs())</span>
            <span id="cb4-13"><a href="#cb4-13" aria-hidden="true"></a><span class="kw">def</span> create_egreedy(epsilon):</span>
            <span id="cb4-14"><a href="#cb4-14" aria-hidden="true"></a>    <span class="co">&quot;Helper method to return a handle for different epsilon values&quot;</span></span>
            <span id="cb4-15"><a href="#cb4-15" aria-hidden="true"></a>    <span class="cf">return</span> <span class="kw">lambda</span> env: e_greedy_selection(env, epsilon)</span>
            <span id="cb4-16"><a href="#cb4-16" aria-hidden="true"></a><span class="kw">def</span> update(epsilon):</span>
            <span id="cb4-17"><a href="#cb4-17" aria-hidden="true"></a>    <span class="kw">global</span> egreedy</span>
            <span id="cb4-18"><a href="#cb4-18" aria-hidden="true"></a>    selection <span class="op">=</span> create_egreedy(epsilon)</span>
            <span id="cb4-19"><a href="#cb4-19" aria-hidden="true"></a>    egreedy <span class="op">=</span> run_simulation(selection, env, n_steps<span class="op">=</span>n_steps, n_trials<span class="op">=</span>n_trials)</span>
            <span id="cb4-20"><a href="#cb4-20" aria-hidden="true"></a>    plot_simulation([random[<span class="dv">1</span>], egreedy[<span class="dv">1</span>]], egreedy[<span class="dv">0</span>], env)</span>
            <span id="cb4-21"><a href="#cb4-21" aria-hidden="true"></a>    plt.show()</span>
            <span id="cb4-22"><a href="#cb4-22" aria-hidden="true"></a><span class="co"># Allows you to interact with different epsilon values and see their effect on solutions relative to the previous random policy</span></span>
            <span id="cb4-23"><a href="#cb4-23" aria-hidden="true"></a>interact_manual(update, epsilon<span class="op">=</span>widgets.FloatSlider(<span class="bu">min</span><span class="op">=</span><span class="dv">0</span>, <span class="bu">max</span><span class="op">=</span><span class="dv">1</span>, step<span class="op">=</span><span class="fl">0.05</span>))</span></code></pre></div>
            <pre><code>interactive(children=(FloatSlider(value=0.0, description=&#39;epsilon&#39;, max=1.0, step=0.05), Button(description=&#39;R…





            &lt;function __main__.update(epsilon)&gt;</code></pre>
            <h3 id="ucb-algorithm">UCB Algorithm</h3>
            <p><span class="math inline">\(\epsilon\)</span>-Greedy works well for a lot of problems, but requires tuning of the hyperparameter is not really principled in any sort of way. This makes it difficult to say anything about the theoretical guarantees of the algorithm. Using some simple arguments about probability bounds, we can find better tradeoffs of exploitation and exploration.</p>
            <p>The idea is to try and estimate how likely a big gap is between our estimated value function and the true one: <span class="math inline">\(Q(a) - \hat{Q}_t(a) \leq \hat{U}_t(a)\)</span>. Hoeffding’s inequality can be used if we do not want to assign any prior to the distribution of rewards, works for any bounded distribution, so for a random variable <span class="math inline">\(r\)</span> bounded between <span class="math inline">\([c_1, c_2]\)</span>:</p>
            <p><span class="math display">\[P[\frac{1}{n} \sum_{i =0}^{n}(r_i - E[r_i]) \geq \Delta\ ] \leq e^{- \frac{2n\Delta^2}{(c_2 - c_1)^2}}\]</span></p>
            <p>This looks eerily similar to our estimates of our value function, <span class="math inline">\(Q(a) = \frac{1}{n} \sum_{i=0}^{n} r_i\)</span>:</p>
            <p><span class="math display">\[P[Q_t - \hat{Q_t} \geq \Delta ] \leq e^{-\frac{2n(a)\Delta^2}{(c_2 - c_1)^2}}\]</span></p>
            <h2 id="ucb-1-heuristic">UCB 1 Heuristic</h2>
            <p>We see that then our upper bound on the true reward can be given by <span class="math inline">\(\Delta = \hat{U_t}(a)\)</span>. And if we make the simplifying assumption that the reward is bounded to be on the interval <span class="math inline">\((0, 1)\)</span>. We can then for any value of the probability of deviation, <span class="math inline">\(p\)</span>, we can solve for what we expect the upper bound on that deviation to be:</p>
            <p><span class="math display">\[
            p \approx e^{-2n(a)U_t(a)^2} \\
            U_t(a) = \sqrt{\frac{- \log p}{2 n (a)}}
            \]</span></p>
            <p>Most of the time in practice, we use a heuristic to drop this <span class="math inline">\(p\)</span> over time, typically <span class="math inline">\(p=t^{-4}\)</span>. This yields the most commonly used version of UCB, UCB-1:</p>
            <p><span class="math display">\[
            U_t(a) = \sqrt{\frac{2 \log t}{n(a)}} \\
            a_t = argmax_a [Q(a) + U_t(a)]
            \]</span></p>
            <div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true"></a><span class="co">&quot;&quot;&quot;</span></span>
            <span id="cb6-2"><a href="#cb6-2" aria-hidden="true"></a><span class="co">UCB-1 action selection gives us a heuristic that works relatively well as long as the arm probabilities are not too close.</span></span>
            <span id="cb6-3"><a href="#cb6-3" aria-hidden="true"></a><span class="co">&quot;&quot;&quot;</span></span>
            <span id="cb6-4"><a href="#cb6-4" aria-hidden="true"></a></span>
            <span id="cb6-5"><a href="#cb6-5" aria-hidden="true"></a><span class="kw">def</span> ucb_selection(env):</span>
            <span id="cb6-6"><a href="#cb6-6" aria-hidden="true"></a>    ucb <span class="op">=</span> np.sqrt((<span class="dv">2</span> <span class="op">*</span> np.log(env.visits.<span class="bu">sum</span>())) <span class="op">/</span> env.visits)</span>
            <span id="cb6-7"><a href="#cb6-7" aria-hidden="true"></a>    mod_avgs <span class="op">=</span> env.arm_avgs() <span class="op">+</span> ucb</span>
            <span id="cb6-8"><a href="#cb6-8" aria-hidden="true"></a>    <span class="cf">return</span> np.argmax(mod_avgs)</span>
            <span id="cb6-9"><a href="#cb6-9" aria-hidden="true"></a></span>
            <span id="cb6-10"><a href="#cb6-10" aria-hidden="true"></a>ucb <span class="op">=</span> run_simulation(ucb_selection, env, n_steps<span class="op">=</span>n_steps, n_trials<span class="op">=</span>n_trials)</span>
            <span id="cb6-11"><a href="#cb6-11" aria-hidden="true"></a>plot_simulation([random[<span class="dv">1</span>], egreedy[<span class="dv">1</span>], ucb[<span class="dv">1</span>]], ucb[<span class="dv">0</span>], env)</span>
            <span id="cb6-12"><a href="#cb6-12" aria-hidden="true"></a>plt.show()</span></code></pre></div>
            <figure>
            <img src="tree_search_files/tree_search_15_0.png" alt="" /><figcaption>png</figcaption>
            </figure>
            <h2 id="part-2-contextual-bandits">Part 2: Contextual Bandits</h2>
            <p>The next class of environments we will look at are referred to as “Contexual” bandit problems. This refers to an observation we get to perform before choosing our action. The observation allows us to have “context” of the situation to use in choosing a useful action.</p>
            <p><img src="assets/3_contextual_bandits.max-1500x1500.png" /></p>
            <p>Examples of these types of problems are very common in recommender systems. We can think of a user having a context (or in a profile) that we can review before making a decision as to what ad we should serve or movie we can recommend.</p>
            <div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true"></a><span class="co">&quot;&quot;&quot;</span></span>
            <span id="cb7-2"><a href="#cb7-2" aria-hidden="true"></a><span class="co">Let&#39;s play around with a bandit to see what the problem is like. You can change any of the variables below</span></span>
            <span id="cb7-3"><a href="#cb7-3" aria-hidden="true"></a><span class="co">to modify how many arms there are or the likelihoods of arms giving reward. Once you run this cell, you will be </span></span>
            <span id="cb7-4"><a href="#cb7-4" aria-hidden="true"></a><span class="co">able to interact with some buttons to produce plots on how well you are doing.</span></span>
            <span id="cb7-5"><a href="#cb7-5" aria-hidden="true"></a><span class="co">&quot;&quot;&quot;</span></span>
            <span id="cb7-6"><a href="#cb7-6" aria-hidden="true"></a><span class="kw">def</span> draw_state(state):</span>
            <span id="cb7-7"><a href="#cb7-7" aria-hidden="true"></a>        <span class="co"># Use a colored box to show context</span></span>
            <span id="cb7-8"><a href="#cb7-8" aria-hidden="true"></a>    <span class="cf">if</span> (state <span class="op">==</span> <span class="dv">0</span>):</span>
            <span id="cb7-9"><a href="#cb7-9" aria-hidden="true"></a>        <span class="cf">return</span> widgets.HTML(value<span class="op">=</span><span class="st">&quot;&lt;div style=&#39;background-color: green ; padding: 10px; border: 1px solid green;&#39;&gt;&quot;</span>)</span>
            <span id="cb7-10"><a href="#cb7-10" aria-hidden="true"></a>    <span class="cf">else</span>: </span>
            <span id="cb7-11"><a href="#cb7-11" aria-hidden="true"></a>        <span class="cf">return</span> widgets.HTML(value<span class="op">=</span><span class="st">&quot;&lt;div style=&#39;background-color: red ; padding: 10px; border: 1px solid red;&#39;&gt;&quot;</span>)</span>
            <span id="cb7-12"><a href="#cb7-12" aria-hidden="true"></a></span>
            <span id="cb7-13"><a href="#cb7-13" aria-hidden="true"></a>n_arms <span class="op">=</span> <span class="dv">8</span></span>
            <span id="cb7-14"><a href="#cb7-14" aria-hidden="true"></a></span>
            <span id="cb7-15"><a href="#cb7-15" aria-hidden="true"></a>n_arms <span class="op">=</span> <span class="dv">2</span></span>
            <span id="cb7-16"><a href="#cb7-16" aria-hidden="true"></a>n_states <span class="op">=</span> <span class="dv">2</span></span>
            <span id="cb7-17"><a href="#cb7-17" aria-hidden="true"></a></span>
            <span id="cb7-18"><a href="#cb7-18" aria-hidden="true"></a><span class="co">## Red: arm 1, Green: arm 0</span></span>
            <span id="cb7-19"><a href="#cb7-19" aria-hidden="true"></a>state_probs <span class="op">=</span> np.array([[<span class="fl">0.1</span>, <span class="fl">0.8</span>], [<span class="fl">0.8</span>, <span class="fl">0.1</span>]])</span>
            <span id="cb7-20"><a href="#cb7-20" aria-hidden="true"></a><span class="co"># state_probs = get_state_probs(n_states, n_arms)</span></span>
            <span id="cb7-21"><a href="#cb7-21" aria-hidden="true"></a>config <span class="op">=</span> {</span>
            <span id="cb7-22"><a href="#cb7-22" aria-hidden="true"></a>    <span class="st">&quot;num_arms&quot;</span> : n_arms,</span>
            <span id="cb7-23"><a href="#cb7-23" aria-hidden="true"></a>    <span class="st">&quot;num_states&quot;</span> : n_states,</span>
            <span id="cb7-24"><a href="#cb7-24" aria-hidden="true"></a>    <span class="st">&quot;state_probs&quot;</span>: state_probs</span>
            <span id="cb7-25"><a href="#cb7-25" aria-hidden="true"></a>}</span>
            <span id="cb7-26"><a href="#cb7-26" aria-hidden="true"></a>env <span class="op">=</span> ContextualBandit(config)</span>
            <span id="cb7-27"><a href="#cb7-27" aria-hidden="true"></a>state <span class="op">=</span> env.reset()</span>
            <span id="cb7-28"><a href="#cb7-28" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;State: </span><span class="sc">{</span>state<span class="sc">}</span><span class="ss">&quot;</span>)</span>
            <span id="cb7-29"><a href="#cb7-29" aria-hidden="true"></a>cumulative_reward <span class="op">=</span> [<span class="dv">0</span>]</span>
            <span id="cb7-30"><a href="#cb7-30" aria-hidden="true"></a></span>
            <span id="cb7-31"><a href="#cb7-31" aria-hidden="true"></a><span class="co"># Create fancy UI</span></span>
            <span id="cb7-32"><a href="#cb7-32" aria-hidden="true"></a>buttons <span class="op">=</span> [widgets.Button(description<span class="op">=</span><span class="ss">f&#39;Arm: </span><span class="sc">{i}</span><span class="ss">&#39;</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_arms)]</span>
            <span id="cb7-33"><a href="#cb7-33" aria-hidden="true"></a>box <span class="op">=</span> draw_state(state)</span>
            <span id="cb7-34"><a href="#cb7-34" aria-hidden="true"></a><span class="kw">def</span> press(button):</span>
            <span id="cb7-35"><a href="#cb7-35" aria-hidden="true"></a>    <span class="kw">global</span> state, cumulative_reward</span>
            <span id="cb7-36"><a href="#cb7-36" aria-hidden="true"></a>    </span>
            <span id="cb7-37"><a href="#cb7-37" aria-hidden="true"></a>    <span class="co">## Sanitize inputs, clear current output</span></span>
            <span id="cb7-38"><a href="#cb7-38" aria-hidden="true"></a>    clear_output()</span>
            <span id="cb7-39"><a href="#cb7-39" aria-hidden="true"></a>    value <span class="op">=</span> <span class="bu">int</span>(button.description.split(<span class="st">&quot; &quot;</span>)[<span class="dv">1</span>])</span>
            <span id="cb7-40"><a href="#cb7-40" aria-hidden="true"></a>    </span>
            <span id="cb7-41"><a href="#cb7-41" aria-hidden="true"></a>    <span class="co">## Step env, keep track of cumulative reward each step</span></span>
            <span id="cb7-42"><a href="#cb7-42" aria-hidden="true"></a>    state, reward <span class="op">=</span> env.step(value)</span>
            <span id="cb7-43"><a href="#cb7-43" aria-hidden="true"></a>    cumulative_reward.append(env.cumulative_reward.<span class="bu">sum</span>())</span>
            <span id="cb7-44"><a href="#cb7-44" aria-hidden="true"></a>    <span class="bu">print</span>(<span class="ss">f&quot;New State: </span><span class="sc">{</span>state<span class="sc">}</span><span class="ss">, Reward: </span><span class="sc">{</span>reward<span class="sc">}</span><span class="ss">, Total steps: </span><span class="sc">{</span>env<span class="sc">.</span>visits<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">}</span><span class="ss">&quot;</span>)</span>
            <span id="cb7-45"><a href="#cb7-45" aria-hidden="true"></a></span>
            <span id="cb7-46"><a href="#cb7-46" aria-hidden="true"></a>    </span>
            <span id="cb7-47"><a href="#cb7-47" aria-hidden="true"></a>    fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">8</span>))</span>
            <span id="cb7-48"><a href="#cb7-48" aria-hidden="true"></a></span>
            <span id="cb7-49"><a href="#cb7-49" aria-hidden="true"></a>    </span>
            <span id="cb7-50"><a href="#cb7-50" aria-hidden="true"></a>    plot_avg_reward_contextual(axs[<span class="dv">0</span>], env)</span>
            <span id="cb7-51"><a href="#cb7-51" aria-hidden="true"></a>    plot_cumulative_reward(axs[<span class="dv">1</span>], cumulative_reward)</span>
            <span id="cb7-52"><a href="#cb7-52" aria-hidden="true"></a>    plot_cumulative_regret(axs[<span class="dv">2</span>], <span class="fl">0.7</span>, env)</span>
            <span id="cb7-53"><a href="#cb7-53" aria-hidden="true"></a>    </span>
            <span id="cb7-54"><a href="#cb7-54" aria-hidden="true"></a>    box <span class="op">=</span> draw_state(state)</span>
            <span id="cb7-55"><a href="#cb7-55" aria-hidden="true"></a>    fig.tight_layout()</span>
            <span id="cb7-56"><a href="#cb7-56" aria-hidden="true"></a>    display(buttons)</span>
            <span id="cb7-57"><a href="#cb7-57" aria-hidden="true"></a>    display(box)</span>
            <span id="cb7-58"><a href="#cb7-58" aria-hidden="true"></a></span>
            <span id="cb7-59"><a href="#cb7-59" aria-hidden="true"></a>[buttons[i].on_click(press) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_arms)]</span>
            <span id="cb7-60"><a href="#cb7-60" aria-hidden="true"></a>buttons <span class="op">=</span> widgets.HBox(buttons)</span>
            <span id="cb7-61"><a href="#cb7-61" aria-hidden="true"></a>display(buttons)</span>
            <span id="cb7-62"><a href="#cb7-62" aria-hidden="true"></a>display(box)</span></code></pre></div>
            <pre><code>New State: 0, Reward: 1, Total steps: 13.0



            HBox(children=(Button(description=&#39;Arm: 0&#39;, style=ButtonStyle()), Button(description=&#39;Arm: 1&#39;, style=ButtonSty…



            HTML(value=&quot;&lt;div style=&#39;background-color: green ; padding: 10px; border: 1px solid green;&#39;&gt;&quot;)</code></pre>
            <figure>
            <img src="tree_search_files/tree_search_17_3.png" alt="" /><figcaption>png</figcaption>
            </figure>
            <div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true"></a><span class="co">&quot;&quot;&quot;</span></span>
            <span id="cb9-2"><a href="#cb9-2" aria-hidden="true"></a><span class="co">Similar as before, using no exploitation gives good coverage across the state/action space.</span></span>
            <span id="cb9-3"><a href="#cb9-3" aria-hidden="true"></a></span>
            <span id="cb9-4"><a href="#cb9-4" aria-hidden="true"></a><span class="co">Note: the heat map on the left is normalized by row to show what the pulls were across all states.</span></span>
            <span id="cb9-5"><a href="#cb9-5" aria-hidden="true"></a><span class="co">&quot;&quot;&quot;</span></span>
            <span id="cb9-6"><a href="#cb9-6" aria-hidden="true"></a>n_steps <span class="op">=</span> <span class="dv">100</span></span>
            <span id="cb9-7"><a href="#cb9-7" aria-hidden="true"></a>n_trials <span class="op">=</span> <span class="dv">500</span></span>
            <span id="cb9-8"><a href="#cb9-8" aria-hidden="true"></a>n_arms <span class="op">=</span> <span class="dv">5</span></span>
            <span id="cb9-9"><a href="#cb9-9" aria-hidden="true"></a>n_states <span class="op">=</span> <span class="dv">10</span></span>
            <span id="cb9-10"><a href="#cb9-10" aria-hidden="true"></a></span>
            <span id="cb9-11"><a href="#cb9-11" aria-hidden="true"></a>n_steps <span class="op">*=</span> n_states <span class="co"># more states dilutes each interaction</span></span>
            <span id="cb9-12"><a href="#cb9-12" aria-hidden="true"></a>state_probs <span class="op">=</span> get_state_probs(n_states, n_arms)</span>
            <span id="cb9-13"><a href="#cb9-13" aria-hidden="true"></a>config <span class="op">=</span> {</span>
            <span id="cb9-14"><a href="#cb9-14" aria-hidden="true"></a>    <span class="st">&quot;num_arms&quot;</span> : n_arms,</span>
            <span id="cb9-15"><a href="#cb9-15" aria-hidden="true"></a>    <span class="st">&quot;num_states&quot;</span> : n_states,</span>
            <span id="cb9-16"><a href="#cb9-16" aria-hidden="true"></a>    <span class="st">&quot;state_probs&quot;</span>: state_probs</span>
            <span id="cb9-17"><a href="#cb9-17" aria-hidden="true"></a>}</span>
            <span id="cb9-18"><a href="#cb9-18" aria-hidden="true"></a>env <span class="op">=</span> ContextualBandit(config)</span>
            <span id="cb9-19"><a href="#cb9-19" aria-hidden="true"></a></span>
            <span id="cb9-20"><a href="#cb9-20" aria-hidden="true"></a><span class="kw">def</span> random_selection(env):</span>
            <span id="cb9-21"><a href="#cb9-21" aria-hidden="true"></a>    <span class="cf">return</span> np.random.randint(env.n_arms)</span>
            <span id="cb9-22"><a href="#cb9-22" aria-hidden="true"></a></span>
            <span id="cb9-23"><a href="#cb9-23" aria-hidden="true"></a>random <span class="op">=</span> run_simulation(random_selection, env, n_steps<span class="op">=</span>n_steps, n_trials<span class="op">=</span>n_trials)</span>
            <span id="cb9-24"><a href="#cb9-24" aria-hidden="true"></a>plot_contextual_simulation([random[<span class="dv">1</span>]], random[<span class="dv">0</span>], env)</span>
            <span id="cb9-25"><a href="#cb9-25" aria-hidden="true"></a>plt.show()</span></code></pre></div>
            <figure>
            <img src="tree_search_files/tree_search_18_0.png" alt="" /><figcaption>png</figcaption>
            </figure>
            <div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true"></a><span class="co">&quot;&quot;&quot;</span></span>
            <span id="cb10-2"><a href="#cb10-2" aria-hidden="true"></a><span class="co">Similar to epsilon greedy as before, we keep track of the average rewards for each state for all the arms. </span></span>
            <span id="cb10-3"><a href="#cb10-3" aria-hidden="true"></a><span class="co">We can then exploit this knowledge every so often dependent on the value of epsilon.</span></span>
            <span id="cb10-4"><a href="#cb10-4" aria-hidden="true"></a><span class="co">&quot;&quot;&quot;</span></span>
            <span id="cb10-5"><a href="#cb10-5" aria-hidden="true"></a><span class="kw">def</span> e_greedy_selection(env, epsilon):</span>
            <span id="cb10-6"><a href="#cb10-6" aria-hidden="true"></a>    <span class="cf">if</span> np.random.rand() <span class="op">&lt;</span> epsilon:</span>
            <span id="cb10-7"><a href="#cb10-7" aria-hidden="true"></a>        <span class="cf">return</span> np.random.randint(env.n_arms)</span>
            <span id="cb10-8"><a href="#cb10-8" aria-hidden="true"></a>    <span class="cf">else</span>:</span>
            <span id="cb10-9"><a href="#cb10-9" aria-hidden="true"></a>        <span class="cf">return</span> np.argmax(env.arm_avgs(env.state))</span>
            <span id="cb10-10"><a href="#cb10-10" aria-hidden="true"></a></span>
            <span id="cb10-11"><a href="#cb10-11" aria-hidden="true"></a><span class="kw">def</span> create_egreedy(epsilon):</span>
            <span id="cb10-12"><a href="#cb10-12" aria-hidden="true"></a>    <span class="cf">return</span> <span class="kw">lambda</span> env: e_greedy_selection(env, epsilon)</span>
            <span id="cb10-13"><a href="#cb10-13" aria-hidden="true"></a></span>
            <span id="cb10-14"><a href="#cb10-14" aria-hidden="true"></a><span class="kw">def</span> update(epsilon):</span>
            <span id="cb10-15"><a href="#cb10-15" aria-hidden="true"></a>    <span class="kw">global</span> egreedy</span>
            <span id="cb10-16"><a href="#cb10-16" aria-hidden="true"></a>    selection <span class="op">=</span> create_egreedy(epsilon)</span>
            <span id="cb10-17"><a href="#cb10-17" aria-hidden="true"></a>    egreedy <span class="op">=</span> run_simulation(selection, env, n_steps<span class="op">=</span>n_steps, n_trials<span class="op">=</span>n_trials)</span>
            <span id="cb10-18"><a href="#cb10-18" aria-hidden="true"></a>    plot_contextual_simulation([random[<span class="dv">1</span>], egreedy[<span class="dv">1</span>]], egreedy[<span class="dv">0</span>], env)</span>
            <span id="cb10-19"><a href="#cb10-19" aria-hidden="true"></a>    plt.show()</span>
            <span id="cb10-20"><a href="#cb10-20" aria-hidden="true"></a></span>
            <span id="cb10-21"><a href="#cb10-21" aria-hidden="true"></a>interact_manual(update, epsilon<span class="op">=</span>widgets.FloatSlider(<span class="bu">min</span><span class="op">=</span><span class="dv">0</span>, <span class="bu">max</span><span class="op">=</span><span class="dv">1</span>, step<span class="op">=</span><span class="fl">0.05</span>))</span></code></pre></div>
            <pre><code>interactive(children=(FloatSlider(value=0.0, description=&#39;epsilon&#39;, max=1.0, step=0.05), Button(description=&#39;R…





            &lt;function __main__.update(epsilon)&gt;</code></pre>
            <div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true"></a><span class="co">&quot;&quot;&quot;</span></span>
            <span id="cb12-2"><a href="#cb12-2" aria-hidden="true"></a><span class="co">As with the other solutions we can simply construct similar estimates as the standard bandit problem, but now stateful.</span></span>
            <span id="cb12-3"><a href="#cb12-3" aria-hidden="true"></a><span class="co">&quot;&quot;&quot;</span></span>
            <span id="cb12-4"><a href="#cb12-4" aria-hidden="true"></a><span class="kw">def</span> ucb_selection(env):</span>
            <span id="cb12-5"><a href="#cb12-5" aria-hidden="true"></a>    ucb <span class="op">=</span> np.sqrt((<span class="dv">2</span> <span class="op">*</span> np.log(env.visits[env.state, :].<span class="bu">sum</span>())) <span class="op">/</span> env.visits[env.state, :])</span>
            <span id="cb12-6"><a href="#cb12-6" aria-hidden="true"></a>    mod_avgs <span class="op">=</span> env.arm_avgs(env.state) <span class="op">+</span> ucb</span>
            <span id="cb12-7"><a href="#cb12-7" aria-hidden="true"></a>    <span class="cf">return</span> np.argmax(mod_avgs)</span>
            <span id="cb12-8"><a href="#cb12-8" aria-hidden="true"></a></span>
            <span id="cb12-9"><a href="#cb12-9" aria-hidden="true"></a>ucb <span class="op">=</span> run_simulation(ucb_selection, env, n_steps<span class="op">=</span>n_steps, n_trials<span class="op">=</span>n_trials)</span>
            <span id="cb12-10"><a href="#cb12-10" aria-hidden="true"></a>plot_contextual_simulation([random[<span class="dv">1</span>], egreedy[<span class="dv">1</span>], ucb[<span class="dv">1</span>]], ucb[<span class="dv">0</span>], env)</span>
            <span id="cb12-11"><a href="#cb12-11" aria-hidden="true"></a>plt.show()</span></code></pre></div>
            <figure>
            <img src="tree_search_files/tree_search_20_0.png" alt="" /><figcaption>png</figcaption>
            </figure>
            <h2 id="part-3-markov-decision-processes-mdps">Part 3: Markov Decision Processes (MDPs)</h2>
            <p>We have been building up the problem description in a more and more complex manner over the course of this session.</p>
            <p>Our agents have only seen a state, took an action, and then recieved a reward for how they acted. There was no notion of time in any of these algorithms and so the agents actions had no effect on the future. Real decision making processes have some degree of effect on the payoff you get. We can model these systems with MDPs.</p>
            <p>MDPs are a general framework for dealing with decision making over some sort of horizon (possibly even infinite).</p>
            <p>For these next set of experiments, its helps intuition to think about making decisions in a spatial manner.</p>
            <div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true"></a>env <span class="op">=</span> FrozenLakeEnv(<span class="dv">8</span>)</span>
            <span id="cb13-2"><a href="#cb13-2" aria-hidden="true"></a>state <span class="op">=</span> env.reset()</span>
            <span id="cb13-3"><a href="#cb13-3" aria-hidden="true"></a>env.render()</span></code></pre></div>
            <figure>
            <img src="tree_search_files/tree_search_23_0.png" alt="" /><figcaption>png</figcaption>
            </figure>
            <h2 id="value-functions">Value Functions</h2>
            <p>Just as with the bandit problems, a value function helped us decide which actions were high quality and those that were not. The only difference between value functions in MDPs and those in Bandits is that added consideration of time. To formalize a bit:</p>
            <p><span class="math display">\[
            Q_t(s, a) = E [ \sum_{k = 0}^{\infty} R_{t + k + 1} | S_t = s, A_t = a]   
            \]</span></p>
            <p>This says the value of a state and an action is what expect the rest of the rewards to be for the rest of time. This is a powerful idea, because it allows us to turn each step of decision making back into a bandit problem.</p>
            <p>Let’s take a look at a value function for the frozen lake environment.</p>
            <div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true"></a><span class="co">&quot;&quot;&quot;</span></span>
            <span id="cb14-2"><a href="#cb14-2" aria-hidden="true"></a><span class="co">The value function can be computed using dynamic programming. If we start from the goal state, we can iterate backwards until we reach the goal.</span></span>
            <span id="cb14-3"><a href="#cb14-3" aria-hidden="true"></a><span class="co">In order to encourage an agent to get to the goal as soon as possible, we discount the reward a state gets the longer it takes to get there.</span></span>
            <span id="cb14-4"><a href="#cb14-4" aria-hidden="true"></a><span class="co">&quot;&quot;&quot;</span></span>
            <span id="cb14-5"><a href="#cb14-5" aria-hidden="true"></a></span>
            <span id="cb14-6"><a href="#cb14-6" aria-hidden="true"></a>value_fcn <span class="op">=</span> value_iteration(env, gamma<span class="op">=</span><span class="fl">0.9</span>)</span>
            <span id="cb14-7"><a href="#cb14-7" aria-hidden="true"></a>env.render()</span>
            <span id="cb14-8"><a href="#cb14-8" aria-hidden="true"></a>plot_value_function(value_fcn, env)</span></code></pre></div>
            <figure>
            <img src="tree_search_files/tree_search_25_0.png" alt="" /><figcaption>png</figcaption>
            </figure>
            <figure>
            <img src="tree_search_files/tree_search_25_1.png" alt="" /><figcaption>png</figcaption>
            </figure>
            <h1 id="monte-carlo-tree-search">Monte Carlo Tree Search</h1>
            <p>We have discussed MCTS on this team quite a bit, but how does it fit into all of this? <img src="assets/EieiQ.png" /></p>
            <p>The algorithm essentially builds up this value function using a combination of tree search + random play (monte carlo simulation)</p>
            <div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true"></a><span class="co"># mcts = MCTS(env)</span></span>
            <span id="cb15-2"><a href="#cb15-2" aria-hidden="true"></a><span class="co"># env.get_action(mcts.next_move(state))</span></span>
            <span id="cb15-3"><a href="#cb15-3" aria-hidden="true"></a></span>
            <span id="cb15-4"><a href="#cb15-4" aria-hidden="true"></a><span class="co"># print(&quot;Value computed during tree search: &quot;)</span></span>
            <span id="cb15-5"><a href="#cb15-5" aria-hidden="true"></a><span class="co"># plot_value_function(mcts.value_fcn(), env)</span></span>
            <span id="cb15-6"><a href="#cb15-6" aria-hidden="true"></a></span>
            <span id="cb15-7"><a href="#cb15-7" aria-hidden="true"></a><span class="bu">print</span>(<span class="st">&quot;Visitations for each state during tree search: &quot;</span>)</span>
            <span id="cb15-8"><a href="#cb15-8" aria-hidden="true"></a>plot_value_function(mcts.visitation_fcn(), env)</span></code></pre></div>
            <pre><code>Visitations for each state during tree search: </code></pre>
            <figure>
            <img src="tree_search_files/tree_search_27_1.png" alt="" /><figcaption>png</figcaption>
            </figure>
            <div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true"></a><span class="co"># state = env.reset()</span></span>
            <span id="cb17-2"><a href="#cb17-2" aria-hidden="true"></a>done <span class="op">=</span> <span class="va">False</span></span>
            <span id="cb17-3"><a href="#cb17-3" aria-hidden="true"></a></span>
            <span id="cb17-4"><a href="#cb17-4" aria-hidden="true"></a><span class="co"># env = FrozenLakeEnv(10)</span></span>
            <span id="cb17-5"><a href="#cb17-5" aria-hidden="true"></a>state <span class="op">=</span> env.reset()</span>
            <span id="cb17-6"><a href="#cb17-6" aria-hidden="true"></a>env.render()</span>
            <span id="cb17-7"><a href="#cb17-7" aria-hidden="true"></a>steps <span class="op">=</span> <span class="dv">0</span></span>
            <span id="cb17-8"><a href="#cb17-8" aria-hidden="true"></a><span class="cf">while</span> <span class="kw">not</span> done:</span>
            <span id="cb17-9"><a href="#cb17-9" aria-hidden="true"></a>    mcts <span class="op">=</span> MCTS(env)</span>
            <span id="cb17-10"><a href="#cb17-10" aria-hidden="true"></a>    action <span class="op">=</span> mcts.next_move(state)</span>
            <span id="cb17-11"><a href="#cb17-11" aria-hidden="true"></a>    state, reward, done, _ <span class="op">=</span> env.step(action)</span>
            <span id="cb17-12"><a href="#cb17-12" aria-hidden="true"></a>    steps <span class="op">+=</span> <span class="dv">1</span></span>
            <span id="cb17-13"><a href="#cb17-13" aria-hidden="true"></a></span>
            <span id="cb17-14"><a href="#cb17-14" aria-hidden="true"></a>    env.render(save<span class="op">=</span><span class="va">True</span>, ind<span class="op">=</span>steps)</span>
            <span id="cb17-15"><a href="#cb17-15" aria-hidden="true"></a>    <span class="co"># plot_value_function(mcts.value_fcn(), env)</span></span>
            <span id="cb17-16"><a href="#cb17-16" aria-hidden="true"></a>    </span>
            <span id="cb17-17"><a href="#cb17-17" aria-hidden="true"></a>create_gif(<span class="st">&quot;assets/frozen_lake/step_&quot;</span>, steps)</span>
            <span id="cb17-18"><a href="#cb17-18" aria-hidden="true"></a></span>
            <span id="cb17-19"><a href="#cb17-19" aria-hidden="true"></a><span class="bu">print</span>(<span class="ss">f&quot;Success! </span><span class="sc">{</span>steps<span class="sc">}</span><span class="ss"> moves&quot;</span>)</span>
            <span id="cb17-20"><a href="#cb17-20" aria-hidden="true"></a></span>
            <span id="cb17-21"><a href="#cb17-21" aria-hidden="true"></a>clear_output(wait<span class="op">=</span><span class="va">True</span>)</span>
            <span id="cb17-22"><a href="#cb17-22" aria-hidden="true"></a>plt.clf()</span>
            <span id="cb17-23"><a href="#cb17-23" aria-hidden="true"></a>Image(url<span class="op">=</span><span class="st">&#39;assets/solution.gif&#39;</span>)  </span></code></pre></div>
            <p><img src="assets/solution.gif"/></p>
            <pre><code>&lt;Figure size 432x288 with 0 Axes&gt;</code></pre>
            <div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"></code></pre></div>
        </main>
    </div>
    <footer>
        <div class="footer-content">
            <div class="footer-section contact">
                <a href="https://github.com/lukeroberto">GitHub</a> |
                <a href="https://linkedin.com/in/lukeroberto">LinkedIn</a>
            </div>
        </div>
        <div class="footer-bottom">
            <p>&copy; 2025 Luke Roberto. All rights reserved.</p>
        </div>
    </footer>
</body>
</html>
